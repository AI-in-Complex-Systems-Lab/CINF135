{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from picamera2 import Picamera2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "camera = Picamera2()\n",
    "config = camera.create_still_configuration(main={\"size\": (300, 300)})\n",
    "camera.configure(config)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We load the MNIST dataset, which contains images of handwritten digits. The dataset is split into training and testing sets. We also apply transformations such as converting images to tensor format and normalizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's print some examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(testloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we define a simple neural network for digit classification. We use a typical structure with fully connected (dense) layers. The training process involves feeding the network with data, calculating loss, and updating model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "losses = []\n",
    "print('Training started. Please wait...')\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        print(f\"Epoch: [{e+1}/{epochs}] | Training loss: {epoch_loss}\")\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After training the model, we evaluate its performance on the test dataset. Metrics like accuracy, precision, and recall are calculated to understand the model's effectiveness in classifying the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section demonstrates a practical application of the trained model. We take a saved image of a handwritten digit, preprocess it to match the input requirements of our model, and then use our model to predict the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_image(image_path, invert=True, contrast=1.5):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    contrast = cv2.convertScaleAbs(gray, alpha=contrast, beta=0)\n",
    "    if invert:\n",
    "        contrast = cv2.bitwise_not(contrast)\n",
    "    blurred = cv2.GaussianBlur(contrast, (9, 9), 0)\n",
    "    _, image = cv2.threshold(blurred, 80, 255, cv2.THRESH_BINARY)\n",
    "    resized = cv2.resize(image, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    # fill the edge pixels with 0\n",
    "    resized[:, [0, -1]] = 0\n",
    "    resized[:, [1, -2]] = 0\n",
    "    resized[[0, -1], :] = 0\n",
    "    resized[[1, -2], :] = 0\n",
    "    return resized\n",
    "\n",
    "\n",
    "def predict_digit(image):\n",
    "    image = TF.to_tensor(image)\n",
    "    image = TF.normalize(image, (0.5,), (0.5,))\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image1 = import_image('data/test.png', invert=True)\n",
    "predicted_digit = predict_digit(image1)\n",
    "\n",
    "image2 = import_image('data/test2.jpg', invert=True)\n",
    "predicted_digit2 = predict_digit(image2)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image1, cmap='gray')\n",
    "plt.title(f'Predicted: {predicted_digit}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image2, cmap='gray')\n",
    "plt.title(f'Predicted: {predicted_digit2}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "1. Try handwriting 3 different numbers on a white paper\n",
    "2. Take a picture of each digit with the camera\n",
    "3. Run the model for each picture \n",
    "4. Write the results\n",
    "5. Calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Image (Re-run this section to capture new images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start with capturing an image with the camera and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.start()\n",
    "camera.capture_file(\"data/capture.jpg\")\n",
    "camera.close()\n",
    "clear_output()\n",
    "\n",
    "\n",
    "image = Image.open(\"data/capture.jpg\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image = import_image('data/capture.jpg', invert=True)\n",
    "predicted_digit = predict_digit(image)\n",
    "\n",
    "plt.imshow(image, cmap='gray', interpolation='none')\n",
    "plt.title(\"Prediction: {}\".format(predicted_digit))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS:\n",
    "   \n",
    "|       | **Actual** | **Predicted** |\n",
    "|:-----:|:----------:|:-------------:|\n",
    "| **1** |      _     |       _       |\n",
    "| **2** |      _     |       _       |\n",
    "| **3** |      _     |       _       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
